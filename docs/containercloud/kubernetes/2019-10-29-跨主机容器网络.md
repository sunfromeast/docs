# 跨主机容器网络

单机环境，在Docker的默认配置下，不同宿主机上的容器通过IP地址进行互相访问时做不到的。为例解决跨主机通信的问题，出现了很多容器网络方案。

## Flannel

Flannel是CoreOS公司主推的网络方案，其本身只是一个框架，真正为我们提供容器网络功能的，是Flannel的后端实现。分别是：

> 1. VXLAN（虚拟可扩展局域网）
> 2. host-gw
> 3. UDP

### UPD

UDP是Flannel最早支持的一种方式，也是性能最差的一种方式，该模式目前已被启用，我们从最简单的说起。

假设，有两台物理机

> 1. `物理机：Node1，容器：container1，容器IP：100.96.1.2，docker0：100.96.1.1/24`
> 2. `物理机：Node2，容器：container2，容器IP：100.96.2.3，docker0：100.96.2.1/24`

假设，container1向container2发送请求：

> 来自容器源地址100.96.1.2的数据，先达到docker0网桥（100.96.1.1/24），由于目的地址100.96.2.3不在docker0网桥的网段范围内，不匹配；所以，会查阅宿主机的路由规则，如下：
>
> ```markdown
> # 在Node 1上
> $ ip route
> default via 10.168.0.1 dev eth0
> 100.96.0.0/16 dev flannel0  proto kernel  scope link  src 100.96.1.0
> 100.96.1.0/24 dev docker0  proto kernel  scope link  src 100.96.1.1
> 10.168.0.0/24 dev eth0  proto kernel  scope link  src 10.168.0.2
> ```
>
> 目的地址和第二条路由规则是匹配的，所以，数据包会进入flannel0设备中（Tunnel设备，在操作系统和用户程序之间传递数据包），flannel0设备会将数据包发给flannel daemon守护进程（从内核态流向用户态），flannel守护进程拿到这个数据包之后，看到目的IP是100.96.2.3，就会把数据包发送Node2上去。

<font color=red>不过，flannel守护进程是怎么知道这个IP地址对应的Pod在Node2上呢？</font>

> flannel的重要概念：子网（subnet）
>
> <font color=purple>flannel管理的容器网络里，它会为每一台宿主机都分配一个子网，每个宿主机上的容器都属于对应的子网。</font>
>
> 子网与宿主机的对应关系，保存在etcd中：
>
> ```markdown
> $ etcdctl ls /coreos.com/network/subnets
> /coreos.com/network/subnets/100.96.1.0-24
> /coreos.com/network/subnets/100.96.2.0-24
> /coreos.com/network/subnets/100.96.3.0-24
> 
> 
> $ etcdctl get /coreos.com/network/subnets/100.96.2.0-24
> {"PublicIP":"10.168.0.3"}
> ```
>
> 拿到了IP，自然就找到了Node2。而后，UDP数据包进入flannel守护进程，flannel守护进程解析出里面的源IP包，将IP包发给flannel0，然后，Linux内核网络栈会查找本机路由处理这个IP包：
>
> ```markdown
> # 在Node 2上
> $ ip route
> default via 10.168.0.1 dev eth0
> 100.96.0.0/16 dev flannel0  proto kernel  scope link  src 100.96.2.0
> 100.96.2.0/24 dev docker0  proto kernel  scope link  src 100.96.2.1
> 10.168.0.0/24 dev eth0  proto kernel  scope link  src 10.168.0.3
> ```
>
> 目标地址是100.96.2.3，和第三条匹配，IP包会进入docker0，而后，进入对应的容器。 

由于用户态和内核态之间的切换有三次，所以，性能不好。而优化的一个重要原则是：减少用户态到内核态的切换次数，把核心的处理逻辑放到内核态进行。，这也是为什么Flannel后来支持VXLAN模式及其成为主流容器网络方案的原因。

### VXLAN

VXLAN （虚拟可扩展局域网）的覆盖网络的设计思想是：在现有的三层网络之上，“覆盖”一层虚拟的、由内核 VXLAN 模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可以）之间，可以像在同一个局域网（LAN）里那样自由通信。当然，实际上，这些“主机”可能分布在不同的宿主机上，甚至是分布在不同的物理机房里。

而为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）。

而 VTEP 设备的作用，其实跟前面的 flanneld 进程非常相似。只不过，它进行封装和解封装的对象，是二层数据帧（Ethernet frame）；而且这个工作的执行流程，全部是在内核里完成的（因为 VXLAN 本身就是 Linux 内核中的一个模块）。

当container1发出请求后，IP包首先到达docker0，然后，进入flannel.1设备进行处理，也就是来到了“隧道”的入口。为了能够将“原始 IP 包”封装并且发送到正确的宿主机，VXLAN 就需要找到这条“隧道”的出口，即：目的宿主机的 VTEP 设备。

而这个设备的信息，正是每台宿主机上的 flanneld 进程负责维护的。

“源 VTEP 设备”收到“原始 IP 包”后，就要想办法把“原始 IP 包”加上一个目的 MAC 地址，封装成一个二层数据帧，然后发送给“目的 VTEP 设备”。

<font color=red>这里需要解决的问题就是：“目的 VTEP 设备”的 MAC 地址是什么？</font>

IP地址在路由表中有，而要根据三层的IP地址查询二层的MAC地址，这正是ARP表的功能。

而这里要用到的 ARP 记录，也是 flanneld 进程在 Node 2 节点启动时，自动添加在 Node 1 上的。我们可以通过 ip 命令看到它，如下所示：

```markdown
# 在Node 1上
$ ip neigh show dev flannel.1
10.1.16.0 lladdr 5e:f8:4f:00:e3:37 PERMANENT
```

拿到了MAC地址，接下来Linux内核就要开始二层封包工作，这个二层帧的格式如下：

![](./images/vxlan1.png)

这个封包只是加了一个二层头，不会改变原始IP包的内容。不过，上面的数据帧（内部数据帧）并不能在宿主机的二层网络中传输，所以，还要把它封装成宿主机网络中普通的数据帧（外部数据帧），让它载着内部数据帧，通过宿主机的etho网卡进行传输。

为了实现这个“搭便车”的机制，Linux 内核会在“内部数据帧”前面，加上一个特殊的 VXLAN 头，用来表示这个“乘客”实际上是一个 VXLAN 要使用的数据帧。

而这个 VXLAN 头里有一个重要的标志叫作 VNI，它是 VTEP 设备识别某个数据帧是不是应该归自己处理的重要标识。而在 Flannel 中，VNI 的默认值是 1，这也是为何，宿主机上的 VTEP 设备都叫作 flannel.1 的原因，这里的“1”，其实就是 VNI 的值。

然后，Linux 内核会把这个数据帧封装进一个 UDP 包里发出去。

不过，一个 flannel.1 设备只知道另一端的 flannel.1 设备的 MAC 地址，却不知道对应的宿主机地址是什么。

在这种场景下，flannel.1 设备实际上要扮演一个“网桥”的角色，在二层网络进行 UDP 包的转发。而在 Linux 内核里面，“网桥”设备进行转发的依据，来自于一个叫作 FDB（Forwarding Database）的转发数据库。

不难想到，这个 flannel.1“网桥”对应的 FDB 信息，也是 flanneld 进程负责维护的。它的内容可以通过 bridge fdb 命令查看到，如下所示：

```markdown
# 在Node 1上，使用“目的VTEP设备”的MAC地址进行查询
$ bridge fdb show flannel.1 | grep 5e:f8:4f:00:e3:37
5e:f8:4f:00:e3:37 dev flannel.1 dst 10.168.0.3 self permanent
```

所以接下来的流程，就是一个正常的、宿主机网络上的封包工作。

![](./images/vxlan2.png)

接下来，Node 1 上的 flannel.1 设备就可以把这个数据帧从 Node 1 的 eth0 网卡发出去。显然，这个帧会经过宿主机网络来到 Node 2 的 eth0 网卡。这时候，Node 2 的内核网络栈会发现这个数据帧里有 VXLAN Header，并且 VNI=1。所以 Linux 内核会对它进行拆包，拿到里面的内部数据帧，然后根据 VNI 的值，把它交给 Node 2 上的 flannel.1 设备。而 flannel.1 设备则会进一步拆包，取出“原始 IP 包”。接下来就回到了我在上一篇文章中分享的单机容器网络的处理流程。最终，IP 包就进入到了 container-2 容器的 Network Namespace 里。